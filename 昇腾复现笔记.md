# 昇腾910B

## Qwen1.5 14B模型测试

[华为昇腾提供文档](https://gitee.com/ascend/ModelLink/blob/master/examples/qwen15/README.md#qwen15-14b)在配置环境过程存在问题，最终直接使用华为提供的环境。

Huggingface数据使用modelscope镜像站链接[下载](https://www.modelscope.cn/models/qwen/Qwen1.5-14B-Chat/files)。

### 模型推理torch模型转华为昇腾

**以下为配置好环境后的操作步骤：**

* 连接VPN

* 连接服务器

  ```bash
  conda activate glm3
  cd ModelLink_backup # 文件部分需要做的是按步骤克隆项目、模型从hf获取
  
  
  # ascend-toolkit 路径
  source /usr/local/Ascend/ascend-toolkit/set_env.sh
  
  # 修改模型权重路径和词表路径
  CHECKPOINT="./model_weights/Qwen1.5-14B-v0.1-tp8-pp1"
  TOKENIZER_PATH="./model_from_hf/Qwen1.5-14B"
  
  # 启动Qwen1.5-14B推理脚本
  bash examples/qwen15/generate_qwen15_14b_ptd.sh
  
  # 得等好一会儿，出现 "You >>" 的时候才可以输入文字
  # ！！！！！！
  # 不知道是不是有bug 别用ctrl+C停程序 用输入quit的方式退出 不然下一次启动有概率会报错！！！！
  ```


### 上游训练

```bash
# 下载数据
cd ./dataset
wget https://huggingface.co/datasets/tatsu-lab/alpaca/resolve/main/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet
cd ..

# 处理数据   
mkdir ./dataset/Qwen1.5-14B/
python ./tools/preprocess_data.py \
    --input ./dataset/train-00000-of-00001-a09b74b3ef9c3b56.parquet \
    --tokenizer-name-or-path ./model_from_hf/Qwen1.5-14B \
    --output-prefix ./dataset/Qwen1.5-14B/alpaca \
    --tokenizer-type PretrainedFromHF \
    --seq-length 8192 \
    --workers 4 \
    --log-interval 1000
    
# 修改examples/qwen15/pretrain_qwen15_14b_ptd.sh
# CKPT_SAVE_DIR="./ckpt/Qwen1.5-14B"
# TOKENIZER_PATH="./model_from_hf/Qwen1.5-14B"  #词表路径
# DATA_PATH="./dataset/Qwen1.5-14B/alpaca_text_document"  #数据集路径
# CKPT_LOAD_DIR="./model_weights/Qwen1.5-14B-v0.1-tp8-pp1"
# TP要修改为8，因为这里是8个模型并行
# 设置 ascend-toolkit 路径
source /usr/local/Ascend/ascend-toolkit/set_env.sh 
bash examples/qwen15/pretrain_qwen15_14b_ptd.sh
```

### 模型finetune

数据集准备

```bash
# 下载数据集
mkdir finetune_dataset
cd ./finetune_dataset
wget https://huggingface.co/datasets/tatsu-lab/alpaca/resolve/main/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet
cd ..

# 处理微调数据集  
mkdir ./finetune_dataset/Qwen1.5-14B/
python ./tools/preprocess_data.py \
    --input ./dataset/train-00000-of-00001-a09b74b3ef9c3b56.parquet \ # 注意修改路径
    --tokenizer-name-or-path ./model_from_hf/Qwen1.5-14B/ \
    --output-prefix ./finetune/Qwen1.5-14B/alpaca \ # 注意修改路径
    --workers 4 \
    --log-interval 1000 \
    --tokenizer-type PretrainedFromHF \
    --handler-name GeneralInstructionHandler \
    --append-eod
#  在model_from_hf/Qwen1.5-14B路径下生成数据
```

**全参微调:** 全参微调的配置脚本基本和预训练脚本一致。

*区别是数据集，以及增加训练参数`--is-instruction-dataset`，增加微调参数`--finetune`，增加预训练权重加载参数`--load` ，使微调从第一步开始}`。*

修改如下：

```bash
CKPT_LOAD_DIR="./model_weights/Qwen1.5-14B-v0.1-tp8-pp1/"
CKPT_SAVE_DIR="./ckpt/Qwen1.5-14B/"
DATA_PATH="./finetune_dataset/Qwen1.5-14B/alpaca"
TOKENIZER_PATH="./model_from_hf/Qwen1.5-14B/"

--load ${CKPT_LOAD_DIR} \
--finetune \
--is-instruction-dataset \
--tokenizer-not-use-fast \ 
在sh脚本里修改
```

下游微调例子：

```bash
# 设置 ascend-toolkit 路径
source /usr/local/Ascend/ascend-toolkit/set_env.sh 

# 根据实际情况配置词表、数据集、模型参数保存路径
CKPT_SAVE_DIR="./ckpt/Qwen1.5-14B"
TOKENIZER_PATH="./model_from_hf/Qwen1.5-14B"  #词表路径
DATA_PATH="./finetune_dataset/Qwen1.5-14B/alpaca"  #数据集路径
CKPT_LOAD_DIR="./model_weights/Qwen1.5-14B-v0.1-tp8-pp1"
```

多机运行增加参数 `--overlap-grad-reduce`。

启动 Qwen1.5-14B 预训练脚本: examples/qwen15/pretrain_qwen15_14b_ptd.sh

```bash
bash examples/qwen15/pretrain_qwen15_14b_ptd.sh
```



# 其它昇腾相关背景知识学习

昇腾Model zoo：

https://www.hiascend.com/software/modelzoo

文档：

https://www.hiascend.com/zh/document

昇腾模型迁移，torch->ascend：

https://www.hiascend.com/document/detail/zh/Pytorch/60RC2/quickstart/fastexperience/fastexperience_0001.html

**MindIE**

MindIE（Mind Inference Engine）是华为昇腾平台推出的推理加速引擎，主要用于优化和加速各种AI推理任务。它支持多种AI框架，包括主流的PyTorch，TensorFlow等。对于大语言模型（LLM）推理，MindIE 提供了专门的组件，叫做 MindIE LLM，能够显著提升大模型推理的性能和易用性。

其中，**MindIE Torch** 是一个专门为 PyTorch 框架优化的插件，允许用户通过简化的接口轻松迁移和优化 PyTorch 模型的推理性能。MindIE 的设计目标是使开发者能够快速地在昇腾平台上构建高性能的推理业务

**MindSpore**：对标torch、tf、pp